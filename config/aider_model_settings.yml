# Aider advanced model settings file
# Format: list of dicts. Each entry targets a "name:" model.
# You can also define global extra params via the special model name: aider/extra_params
#
# This file is meant to support:
# - multi-provider / multi-model usage
# - consistent edit formats
# - stable defaults (streaming, repo map usage, etc.)
#
# Usage example:
# aider --model-settings-file config/aider_model_settings.yml --model <model-name> ...

- name: aider/extra_params
  extra_params:
    # Common defaults sent to litellm.completion() when supported by provider/model
    max_tokens: 4096
    # You can add headers or other provider-specific params here.
    # extra_headers:
    #   X-Custom-Header: value

# --- Local (Ollama) fast model for planning / lightweight edits ---
- name: ollama/llama3.1
  edit_format: diff
  use_repo_map: true
  use_system_prompt: true
  streaming: true
  use_temperature: true
  extra_params:
    temperature: 0.6
    max_tokens: 2048

# --- Stronger local model (if you run it) ---
- name: ollama/qwen2.5-coder
  edit_format: diff
  use_repo_map: true
  use_system_prompt: true
  streaming: true
  use_temperature: true
  extra_params:
    temperature: 0.4
    max_tokens: 4096

# --- OpenAI (API) general strong ---
- name: openai/gpt-4o
  edit_format: diff
  use_repo_map: true
  use_system_prompt: true
  streaming: true
  use_temperature: true
  extra_params:
    temperature: 0.4
    max_tokens: 4096

# --- OpenAI (API) fast/cheap ---
- name: openai/gpt-4o-mini
  edit_format: diff
  use_repo_map: true
  use_system_prompt: true
  streaming: true
  use_temperature: true
  extra_params:
    temperature: 0.5
    max_tokens: 2048

# --- Anthropic (API) strong ---
- name: anthropic/claude-3-7-sonnet
  edit_format: diff
  use_repo_map: true
  use_system_prompt: true
  streaming: true
  use_temperature: true
  extra_params:
    temperature: 0.4
    max_tokens: 4096

# --- DeepSeek (API) option ---
- name: deepseek/deepseek-chat
  edit_format: diff
  use_repo_map: true
  use_system_prompt: true
  streaming: true
  use_temperature: true
  extra_params:
    temperature: 0.4
    max_tokens: 4096
