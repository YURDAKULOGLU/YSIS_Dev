# Local Modelleri Pratikte Kullanma Rehberi

**OluÅŸturulma:** 2025-11-26T20:16:00+03:00
**Hedef Sistem:** RTX 5090 + Ryzen 9 9950X3D + 64GB RAM
**Durum:** ğŸŸ¢ Sistemde 5 model hazÄ±r

---

## ğŸ¯ 3 Pratik KullanÄ±m YÃ¶ntemi

### YÃ¶ntem 1: Terminal'den Direkt KullanÄ±m (En Basit)

**Senaryo:** HÄ±zlÄ± kod Ã¼retimi, soru-cevap

```bash
# Model baÅŸlat ve soru sor
ollama run deepseek-r1:32b "Write a TypeScript interface for a User with id, name, email, and created_at fields"

# BaÅŸka bir model
ollama run qwen2.5:14b "Explain how React useEffect works"

# KÄ±sa soru iÃ§in hÄ±zlÄ± model
ollama run llama3 "What is the difference between interface and type in TypeScript?"
```

**Avantajlar:**
- âœ… AnÄ±nda cevap
- âœ… Kurulum gerektirmez
- âœ… API key yok
- âœ… SÄ±nÄ±rsÄ±z kullanÄ±m

**Dezavantajlar:**
- âŒ Her seferinde model yÃ¼klenir (yavaÅŸ)
- âŒ Conversation history yok
- âŒ Dosyalarla etkileÅŸim yok

---

### YÃ¶ntem 2: API Modu (Ã–nerilen - En Esnek)

**Senaryo:** Script'lerden, VS Code extension'lardan, diÄŸer tool'lardan kullanÄ±m

#### AdÄ±m 1: Ollama Server BaÅŸlat
```bash
# Terminal 1'de - Server'Ä± baÅŸlat (arka planda Ã§alÄ±ÅŸÄ±r)
ollama serve
```

#### AdÄ±m 2: VS Code'da Kullan
```json
// .vscode/settings.json
{
  "continue.models": [
    {
      "title": "DeepSeek Coder",
      "provider": "ollama",
      "model": "deepseek-r1:32b",
      "apiBase": "http://localhost:11434"
    }
  ]
}
```

#### AdÄ±m 3: Python Script'ten Kullan
```python
# generate_tests.py
import requests
import json

def generate_unit_tests(code_file):
    with open(code_file, 'r') as f:
        code = f.read()

    prompt = f"Generate comprehensive Jest unit tests for this TypeScript code:\n\n{code}"

    response = requests.post('http://localhost:11434/api/generate',
        json={
            'model': 'deepseek-r1:32b',
            'prompt': prompt,
            'stream': False
        })

    return response.json()['response']

# KullanÄ±m
tests = generate_unit_tests('src/hooks/useUserContext.tsx')
print(tests)
```

#### AdÄ±m 4: Node.js Script'ten Kullan
```javascript
// generate_docs.js
import ollama from 'ollama';

async function generateDocs(filePath) {
  const code = await fs.readFile(filePath, 'utf-8');

  const response = await ollama.generate({
    model: 'qwen2.5:14b',
    prompt: `Generate comprehensive JSDoc comments for this TypeScript code:\n\n${code}`,
  });

  return response.response;
}

// KullanÄ±m
const docs = await generateDocs('src/services/ai/tools.ts');
console.log(docs);
```

**Avantajlar:**
- âœ… Server bir kez baÅŸlar, hep hazÄ±r
- âœ… OpenAI API ile uyumlu
- âœ… Scriptlerden kolay eriÅŸim
- âœ… Conversation history tutulabilir
- âœ… Streaming support

**Dezavantajlar:**
- âŒ Server'Ä±n aÃ§Ä±k kalmasÄ± gerekir

---

### YÃ¶ntem 3: Workflow Automation (En GÃ¼Ã§lÃ¼)

**Senaryo:** Multi-agent sistemde otomatik gÃ¶rev daÄŸÄ±tÄ±mÄ±

#### Ã–rnek Workflow: Test Generation

```bash
# workflow-test-generation.sh

#!/bin/bash

echo "ğŸ§ª Automated Test Generation Workflow"
echo "======================================"

# 1. DeÄŸiÅŸen dosyalarÄ± bul
CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD | grep ".tsx\|.ts")

for FILE in $CHANGED_FILES; do
    echo "ğŸ“ Generating tests for: $FILE"

    # 2. Dosya iÃ§eriÄŸini oku
    CODE=$(cat $FILE)

    # 3. DeepSeek ile test Ã¼ret
    TESTS=$(ollama run deepseek-r1:32b "Generate Jest unit tests for this TypeScript code. Only output the test code:\n\n$CODE")

    # 4. Test dosyasÄ± oluÅŸtur
    TEST_FILE="${FILE%.tsx}.test.tsx"
    echo "$TESTS" > "$TEST_FILE"

    echo "âœ… Created: $TEST_FILE"
done

echo "ğŸ‰ Test generation complete!"
```

#### Ã–rnek Workflow: Documentation Update

```bash
# workflow-update-docs.sh

#!/bin/bash

echo "ğŸ“š Automated Documentation Update"
echo "=================================="

# 1. TÃ¼m servis dosyalarÄ±nÄ± bul
SERVICE_FILES=$(find src/services -name "*.ts" -not -name "*.test.ts")

for FILE in $SERVICE_FILES; do
    echo "ğŸ“– Updating docs for: $FILE"

    # 2. Qwen ile dokÃ¼mantasyon Ã¼ret
    DOCS=$(ollama run qwen2.5:14b "Add comprehensive JSDoc comments to this code. Return the full code with comments:\n\n$(cat $FILE)")

    # 3. DosyayÄ± gÃ¼ncelle
    echo "$DOCS" > "$FILE"

    echo "âœ… Updated: $FILE"
done

echo "ğŸ‰ Documentation update complete!"
```

#### Ã–rnek Workflow: Code Review

```bash
# workflow-code-review.sh

#!/bin/bash

echo "ğŸ” Automated Code Review"
echo "========================"

# 1. PR'daki deÄŸiÅŸiklikleri al
DIFF=$(git diff main...HEAD)

# 2. Gemini (veya baÅŸka model) ile review yap
REVIEW=$(ollama run mixtral:latest "Review this code diff. Check for:\n- YBIS Constitution violations\n- TypeScript best practices\n- Potential bugs\n- Performance issues\n\nDiff:\n$DIFF")

# 3. Review'u markdown'a kaydet
echo "# Automated Code Review - $(date)" > review.md
echo "" >> review.md
echo "$REVIEW" >> review.md

# 4. Slack/Discord'a gÃ¶nder (opsiyonel)
# curl -X POST -H 'Content-type: application/json' --data "{\"text\":\"$REVIEW\"}" $SLACK_WEBHOOK

echo "âœ… Review saved to review.md"
cat review.md
```

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§: En Pratik Setup

### 1. Ollama Server'Ä± Arka Planda BaÅŸlat

```bash
# Windows'ta (PowerShell)
Start-Process -NoNewWindow ollama serve

# Linux/Mac'te
nohup ollama serve &
```

### 2. VS Code Extension Kur (Continue.dev)

```bash
# VS Code'da
# Extensions -> "Continue" ara -> Install
# Sonra settings.json'a ekle:
{
  "continue.models": [
    {
      "title": "DeepSeek 32B (Heavy)",
      "model": "deepseek-r1:32b",
      "provider": "ollama"
    },
    {
      "title": "Qwen 14B (Medium)",
      "model": "qwen2.5:14b",
      "provider": "ollama"
    },
    {
      "title": "Llama 8B (Fast)",
      "model": "llama3",
      "provider": "ollama"
    }
  ]
}
```

### 3. Package Scripts Ekle

```json
// package.json
{
  "scripts": {
    "ai:test": "node scripts/ai-generate-tests.js",
    "ai:docs": "node scripts/ai-update-docs.js",
    "ai:review": "bash scripts/ai-code-review.sh",
    "ai:refactor": "node scripts/ai-suggest-refactor.js"
  }
}
```

---

## ğŸ’¡ Pratik KullanÄ±m SenaryolarÄ±

### Senaryo 1: Unit Test Ãœretimi
```bash
# Manuel
ollama run deepseek-r1:32b < useUserContext.tsx > useUserContext.test.tsx

# Otomatik (git hook)
# .git/hooks/pre-commit
npm run ai:test
```

### Senaryo 2: API DokÃ¼mantasyonu
```bash
# TÃ¼m API route'larÄ± iÃ§in swagger docs Ã¼ret
find apps/backend/src/routes -name "*.ts" | while read file; do
    ollama run qwen2.5:14b "Generate OpenAPI/Swagger documentation for this Express route: $(cat $file)" > "${file%.ts}.swagger.yaml"
done
```

### Senaryo 3: Code Review AsistanÄ±
```bash
# PR'daki her commit iÃ§in review
git log main..HEAD --pretty=format:"%H" | while read commit; do
    DIFF=$(git show $commit)
    ollama run mixtral "Review this commit for YBIS standards:\n$DIFF" > "reviews/$commit.md"
done
```

### Senaryo 4: Refactoring Ã–nerileri
```bash
# KarmaÅŸÄ±k dosyalar iÃ§in refactoring Ã¶nerisi
find src -name "*.tsx" -exec wc -l {} \; | sort -rn | head -10 | while read lines file; do
    if [ $lines -gt 200 ]; then
        ollama run deepseek-r1:32b "Suggest refactoring for this large file (${lines} lines): $(cat $file)" > "refactor-suggestions/$(basename $file).md"
    fi
done
```

---

## âš¡ Performance Optimizasyonu

### Hot Loading (Ã–nerilen)
```bash
# Model'i VRAM'e yÃ¼kle ve orada tut
ollama run deepseek-r1:32b "" &
ollama run qwen2.5:14b "" &

# Åimdi her Ã§aÄŸrÄ± HIZLI (model zaten yÃ¼klÃ¼)
# 1. Ã§aÄŸrÄ±: 30 saniye (model loading)
# 2+ Ã§aÄŸrÄ±lar: 2 saniye (sadece inference)
```

### Parallel Processing
```bash
# 3 task'Ä± paralel Ã§alÄ±ÅŸtÄ±r
(ollama run deepseek-r1:32b "Generate tests for file1" > file1.test.tsx) &
(ollama run qwen2.5:14b "Generate docs for file2" > file2.docs.md) &
(ollama run llama3 "Explain file3" > file3.explanation.txt) &

wait
echo "All done!"
```

### Batch Processing
```bash
# BirÃ§ok dosyayÄ± toplu iÅŸle
find src -name "*.tsx" | xargs -P 3 -I {} bash -c 'ollama run qwen2.5:14b "Add comments to: $(cat {})" > {}.commented'
```

---

## ğŸ¯ Agentic Workflow Entegrasyonu

### Task Board'dan Otomatik GÃ¶rev Alma

```javascript
// agent-worker-local.js
const fs = require('fs');
const ollama = require('ollama');

async function checkTaskBoard() {
    const taskBoard = fs.readFileSync('shared/TASK_BOARD.md', 'utf-8');

    // "P2: Generate unit tests" gibi bir task bul
    const match = taskBoard.match(/\[ \] \*\*P2: Generate unit tests for `(.+)`\*\*/);

    if (match) {
        const file = match[1];
        console.log(`ğŸ“ Found task: Generate tests for ${file}`);

        // DeepSeek ile test Ã¼ret
        const code = fs.readFileSync(file, 'utf-8');
        const response = await ollama.generate({
            model: 'deepseek-r1:32b',
            prompt: `Generate Jest unit tests:\n\n${code}`
        });

        // Test dosyasÄ± oluÅŸtur
        const testFile = file.replace('.tsx', '.test.tsx');
        fs.writeFileSync(testFile, response.response);

        // Communication log'a yaz
        const log = `\n### [AGENT: DeepSeek-R1] [TIME: ${new Date().toISOString()}]
**Status:** âœ… COMPLETED
**Task:** Generate unit tests for ${file}
**Output:** ${testFile}
**Lines Generated:** ${response.response.split('\n').length}
**Duration:** ${response.total_duration / 1e9}s\n`;

        fs.appendFileSync('communication_log.md', log);

        console.log(`âœ… Completed: ${testFile}`);
    }
}

// Her 5 dakikada bir kontrol et
setInterval(checkTaskBoard, 5 * 60 * 1000);
```

---

## ğŸ“Š Model SeÃ§im Rehberi

### DeepSeek-R1 32B (19GB)
**Ne zaman kullan:**
- âœ… Complex coding tasks
- âœ… Multi-file refactoring planlarÄ±
- âœ… Architecture decisions
- âœ… Test generation (comprehensive)
- âŒ Basit sorular iÃ§in overkill

**Ã–rnek:**
```bash
ollama run deepseek-r1:32b "Refactor this 500-line component into smaller, reusable pieces"
```

### Qwen2.5 14B (9GB)
**Ne zaman kullan:**
- âœ… Documentation generation
- âœ… Code explanation
- âœ… Medium complexity coding
- âœ… API design
- âŒ Quick questions iÃ§in yavaÅŸ

**Ã–rnek:**
```bash
ollama run qwen2.5:14b "Generate comprehensive JSDoc for this service class"
```

### Llama3 8B (4.7GB)
**Ne zaman kullan:**
- âœ… Quick questions
- âœ… Simple code generation
- âœ… Explanation requests
- âœ… Code formatting
- âŒ Complex reasoning iÃ§in yetersiz

**Ã–rnek:**
```bash
ollama run llama3 "What does this function do?"
```

### Mixtral (26GB)
**Ne zaman kullan:**
- âœ… Code review
- âœ… Heavy reasoning tasks
- âœ… Multi-language tasks
- âœ… Architecture analysis
- âŒ VRAM'de yer kaplar

**Ã–rnek:**
```bash
ollama run mixtral "Review this PR for security vulnerabilities"
```

---

## ğŸ”¥ Ä°lk 3 Pratik AdÄ±m (BugÃ¼n YapÄ±labilir)

### 1. Server'Ä± BaÅŸlat (2 dakika)
```bash
ollama serve
```

### 2. Ä°lk Test'i Ãœret (5 dakika)
```bash
ollama run deepseek-r1:32b "Generate Jest unit tests for this hook: $(cat apps/mobile/src/hooks/useUserContext.tsx)" > test-output.txt
cat test-output.txt
```

### 3. Script OluÅŸtur (10 dakika)
```bash
# scripts/ai-generate-tests.sh
#!/bin/bash
FILE=$1
ollama run deepseek-r1:32b "Generate Jest tests: $(cat $FILE)" > "${FILE%.tsx}.test.tsx"
echo "âœ… Generated: ${FILE%.tsx}.test.tsx"
```

**KullanÄ±m:**
```bash
bash scripts/ai-generate-tests.sh src/hooks/useUserContext.tsx
```

---

## ğŸ’° Maliyet KarÅŸÄ±laÅŸtÄ±rmasÄ±

| YÃ¶ntem | Maliyet | HÄ±z | SÄ±nÄ±rlama |
|--------|---------|-----|-----------|
| **OpenAI GPT-4** | $0.03/1K tokens | HÄ±zlÄ± | Rate limits |
| **Claude Sonnet** | $0.015/1K tokens | HÄ±zlÄ± | Rate limits |
| **Local (Ollama)** | **$0** | Orta | GPU/RAM |

**Ã–rnek Hesap:**
- 1000 test case Ã¼retimi
- GPT-4: ~$30
- Ollama: **$0**
- **KazanÃ§: $30 Ã— 12 ay = $360/yÄ±l**

---

## ğŸ¯ Sonraki AdÄ±mlar

1. **BugÃ¼n:** `ollama serve` baÅŸlat, bir test Ã¼ret
2. **Bu Hafta:** 3 automation script yaz
3. **Gelecek Hafta:** Agentic workflow'a entegre et
4. **Ay Sonuna:** 30% task'larÄ± local modeller yapsÄ±n

---

**OluÅŸturan:** @GitHub Copilot CLI
**GÃ¼ncelleme:** 2025-11-26T20:16:00+03:00
**Durum:** ğŸŸ¢ Ready to use
